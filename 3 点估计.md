# 3.1 引言

参数估计是统计推断的一种重要形式，是利用样本对已知分布形式的分布族中的未知参数进行估计。

参数估计分为点估计和区间估计。

使用样本点的函数 $\hat g(\boldsymbol X)$ 作为 $g(\theta)$ 的估计，称为点估计。

评价点估计的优良性的准则包括无偏性、有效性、相合性、渐近正态性等。

- 无偏性要求 $\mathrm E_\theta(\hat g(\boldsymbol X))=g(\theta)$，即估计量的期望应当就是被估计的数。
	- 样本方差是总体方差的一个无偏估计
- 有效性要求估计量的方差越小越好
- 相合性要求估计量与真实值之间的偏差随样本容量的增加逐渐降低
- 渐进正态性

# 3.2 矩估计

基本思想是要求样本的 k 阶矩等于总体的 k 阶矩。使用原点矩还是中心矩依照情况来定。

一般来说用样本的原点矩估计总体的原点矩是无偏的，但是中心矩估计一般是有偏的，比如 $S_n^2=\frac{\sum_i (X_i-\overline X)^2}{n}$ 的期望为 $\frac{n-1}{n}\sigma^2$.

如果要估计的量可以表示为总体分布的某些矩的函数，就用对应的样本矩代替总体矩进行估计，得到的估计值称为矩估计。由于将一个量表示为若干矩的方法不唯一，所以矩估计也不唯一。

矩估计的优点在于比较直观、不需要对总体的分布有很多的认识、比较简便易行；缺点在于不能很好地利用参数分布族的信息，并且不具有唯一性。

样本原点矩是总体原点矩的无偏估计，而样本中心矩对总体中心矩的估计一定（？）是有偏的。对于待估函数是一些矩的函数的情况，一般矩估计是有偏的，但是如果该函数是矩的线性组合则一般是无偏的。

即使是有偏的矩估计一般也具有渐进无偏性。（比如 $S_n^2$）

矩估计还具有相合性和渐进正态性。

*渐进正态性这里似乎还是主要了解 delta 方法即可。不需要了解到底是怎样 converge 的，比如殆必收敛和依概率收敛之间的区别等。*

# 3.3 极大似然估计

将概率函数 $f(\boldsymbol x;\theta)$ 看作 $\theta$ 的函数，即为似然函数（likelihood function）。使得似然函数取极大值的估计方法称为极大似然估计。

此处有很多关于极大似然估计的例子。比较有趣的是位置未定的均匀分布的 MLE 有无穷多个，只要 $\theta$ 取值使得最大值和最小值都有可能取到即可。

极大似然估计可能是有偏的。

极大似然估计一定可以表示为充分统计量的函数。

极大似然估计具有渐近正态性，方差为 Fisher 信息量的倒数。

$$
\sqrt{n}(\hat{\theta}^*-\theta)\xrightarrow{\mathscr{L}}N\Big(0,\frac1{I(\theta)}\Big)
$$

# 3.4 一致最小方差无偏估计

评价估计量的好坏可以用均方误差（MSE）来描述，定义为

$$
\mathrm{MSE}=E_\theta(\hat{g}(\boldsymbol{X})-g(\theta))^2
$$

均方差等于 bias 平方和估计量本身方差的和。

在实际情况中一致最小均方估计常常是不存在的，退而求其次，在所有无偏估计量中找到方差最小的统计量是可行的。

存在不存在无偏估计的情况，比如未知 $p$ 的二项分布中求 $1/p$ 的估计。

对于同样的无偏估计，我们计算其中方差最小的估计量，即 **一致最小方差无偏估计**（UMVUE）.

以下是一个改进无偏估计的方法：如果 $\hat g(\boldsymbol X)$ 是一个无偏估计，可以用充分统计量 $T(\boldsymbol X)$ 对这个估计进行改进，定义 $h(T)=E(\hat{g}(\boldsymbol{X})|T)$，则 $h(T)$ 也是无偏估计，并且只要是非平凡的函数，方差都小于原估计。可以理解为任何无偏估计对充分统计量的条件期望都可以导出一个新的无偏估计，并且只要原估计不是 $T(\boldsymbol X)$ 的函数，那么方差一定会变小。

这个定理还表明，UMVUE 一定是充分统计量的函数。（*如果没有“最小”二字，这句话毫无意义*）（一般来说直接在常见的充分统计量基础上构造一个无偏估计，就大概率是 UMVUE）

## 零无偏估计法

主要为了判定一个估计是否是 UMVUE，不能用来寻找 UMVUE。

一个无偏估计 $\hat g(\boldsymbol X)$ 是 UMVUE，当且仅当对任何满足“对于任何 $\theta$ 都有 $E_\theta l(\boldsymbol X)=0$”的函数 $l(\boldsymbol X)$ 都有“对于任何 $\theta$ 都有 $\mathrm{Cov}(\hat g(\boldsymbol X),l(\boldsymbol X))=0$”，即这个估计和任何 0 的无偏估计都不相关。证明直接计算任何无偏估计的方差即可。

这个定理的适用不太方便，因为零无偏估计很多。

如果无偏估计是充分统计量 T 的函数，那么只要对于任何导出自充分统计量的 0 的无偏估计 $\delta(T)$ 都有协方差为 0，那么即可证明这个无偏估计是 UMVUE。即可以用充分统计量 T 替代样本 X。

二项分布的例题中似乎采用证明 T 为完备统计量的方式来证明 $\delta(\boldsymbol X)$ 为 0，那么其协方差一定是 0。所以任何充分完备统计量的函数只要无偏，那么其一定为 UMVUE。这里的 UMVUE 是 $\overline X$.

均匀分布也是类似的，对参数求导可知其为完备统计量。这里的 UMVUE 是 $\frac{n+1}n X_{(n)}$

对于指数分布来说，也可以根据 Laplace 变换证明其完备性，或者对参数求导之后的结果恰好可以证明两个统计量积的期望为 0. 总体均值 $1/\lambda$ 的 UMVUE 是 $\overline X$.

正态分布与指数分布类似，可以通过求导等手段构造出二者积的期望。

*这里再回顾一下完备统计量：对于完备统计量 T，不存在从 T 导出的函数是 0 的无偏估计，除非 T 是恒为 0 的。*

## 充分完全统计量法（Lehmann-Scheff 定理）

由充分且完备统计量构造出的无偏估计 **一定是唯一的** UMVUE！

进一步地，如果是指数分布族，那么由指数部分中每一个 $T_i(\boldsymbol X)$ 构造的无偏估计都是唯一的，前提是自然参数空间作为 $\mathbb R^k$ 的子集存在内点。（完备性的要求）

*这样一来零无偏估计法似乎没有太大的价值？*

除太过显然的直接证明之外，LS 定理的例题主要是寻找无偏统计量的过程。难点在于直接从充分且完备统计量出发构造出无偏统计量，直接用统计量替换参数得到的结果很容易变成有偏的，尤其是非线性情况下。

比如例 3.4.8：根据 Bernoulli 试验进行 n 次得到的样本估计 $g(p)=p(1-p)$ 的大小，最后的 UMVUE 是 $\frac{t(n-t)}{n(n-1)}$，其中 $t=\overline X$ 是充分且完备的统计量。

方法 1：在已知充分完备统计量的前提下，先构造一个比较显然的无偏估计，然后用充分统计量加以改进，最后得到的量一定是充分且完备统计量的函数，自然也就是 UMVUE。

方法 2：直接嗯构造，猜测一个函数形式是 $\delta(t)$，根据无偏的条件进行化简。

*这里的两种方法在上课的 PPT 中也有提及。*

*还有第三种方法，先写出充分完备统计量的某个幂次的期望，然后进行一定的线性调整，常见于正态情况。*

例 3.4.9：Poisson 分布中 $\lambda, \lambda^r, P(X_i=x)$ 三个量的无偏估计，其中 $t=\sum_i X_i$ 是充分且完备统计量。第一个量的估计即为样本均值，而第二个量的估计与上一例题中的方法 2 类似，都是直接根据无偏条件求合适的充分且完备统计量的函数；第三个量的估计用的则是先求无偏估计，再根据充分统计量优化的方法。

例 3.4.10 根据指数分布的随机样本对 $\lambda$ 和 $1-\mathrm e^{-\lambda x_0}$ 求 UMVUE.

例 3.4.11 求正态分布中 $a,\sigma^2,\sigma^r,a/\sigma^2$ 等的 UMVUE. 其中最后一个量的估计可以根据正态样本中样本均值和样本方差独立来直接计算，但是仍然需要计算 $E(S^2)$ 的值。对于正态分布，要合理利用样本均值 $\overline X$ 和样本方差 $S^2$ 所满足的分布这一信息。

# 3.5 Cramer-Rao 不等式

CR 不等式可以给出无偏估计量的下界，如果一个无偏估计的方差达到这个下界，那么这个无偏估计就是一个 UMVUE. 在很多情况下，即使是 UMVUE 也无法达到 CR 不等式给出的下界，此时 CR 不等式无法对 UMVUE 进行判断。

CR 不等式需要一些正则条件

- 参数空间是开区间；
- 分布族具有共同的支撑集，即取值范围不依赖于参数；
- 概率函数对参数 $\theta$ 的导数存在；
- 概率函数的积分和微分可交换；
- Fisher 信息量存在

则任何无偏估计 $\hat g(\boldsymbol X)$ 的方差不大于某一值：

$$
D_\theta[\hat{g}(\boldsymbol{X})]\geqslant\frac{(g'(\theta))^2}{nI(\theta)},\quad\forall\theta\in\Theta.
$$

如果 $g(\theta)=\theta$，那么 $D_\theta[\hat{g}(\boldsymbol{X})]\geqslant\frac{1}{nI(\theta)}$ 

对于指数族，正则条件全部成立。

需要详细看一下例 3.5.1-4.

看了一下，就是硬算，没什么技术含量。

## Fisher 信息量

信息量越大，无偏估计的方差的下界越低，说明对参数 $\theta$ 的估计能够越精确。

Fisher 信息量还出现在极大似然估计的渐进方差中。

## 估计的效率

将比值 $e_\hat g(\theta)$ 称为估计 $\hat g$ 的效率（efficiency）

$$
e_{\hat{g}}(\theta)=\frac{[g'(\theta)]^2/(nI(\theta))}{D_\theta[\hat{g}(\boldsymbol{X})]}
$$

即该估计在多大程度上达到了 CR 不等式给出的下界。效率为 1 的无偏估计称为有效估计。